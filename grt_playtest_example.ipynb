{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LLM Scavenger Hunt\n",
    "\n",
    "We want end users to learn some prompt engineering and limitations of Large Language Models. We want this to be fun and simple to interact with, so that someone at a busy con or event can come by and try one or two challenges and learn something, while still being deep enough to actually teach people who want these skills.\n",
    "\n",
    "There will be a jeopardy board of challenges. For example a person might be presented with \"Secret Key\" for 10 points. If they click on it they will have a description telling them that they are interacting with a chatbot that was instructed not to reveal it's name and a text box where they can enter a prompt. Their task is to get it to reveal it's name. \n",
    "\n",
    "## How this works.\n",
    "\n",
    "What we need for each challenge is:\n",
    "1. A description of the challenge.\n",
    "2. The secret prompt for the challenge. This can be empty.\n",
    "3. A prompt that can solve this challenge.\n",
    "4. The difficulty of the challenge.\n",
    "\n",
    "The users prompt will be appended to the challenge's secret prompt and then sent to the LLM. It then generates some tokens (capped at 20 in dev, but it will be extended), and returns them to the participants. If they are happy they can submit to the judges for scoring. We're using a custom plugin for [CTFd](https://github.com/CTFd/CTFd) do accomplish this. \n",
    "\n",
    "## Models\n",
    "\n",
    "All prompts during the contest will be directed at GPT-NeoX-20b for now. We will host Bloom eventually, and some fine tuned models with specific tasks. So, try to make challenges that work with foundational models. If there's a challenge specific to a small to medium model (under 20b parameters) that you want to target, ask Sven if it can be hosted. The challenge infra will be able to handle this. \n",
    "\n",
    "# API Keys\n",
    "\n",
    "The following code block sets up the client. You can use the `huggingface_service.py` with a raw python invocation or with the docker container using `docker-compose up --build` in this repo to host a model locally. We have a hosted NeoX-20b that Sven can give you API keys for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from remote_llm.client import ClientLLM\n",
    "\n",
    "api_key = \"482fdd5f-b59c-43de-98b9-4e19a21b4d85\"\n",
    "host = \"localhost\"\n",
    "port = 50055\n",
    "\n",
    "client = ClientLLM(api_key=api_key, host=host, port=port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "Here we generate some text to see if this works. Edit the prompt and secret prompt. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' As he got his attention, he saw a fox staring at him.  It was not that of'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret_prompt = \"\" # This is a secret prompt that the user doesn't see.\n",
    "prompt = \"The quick brown fox jumped over the lazy dog. \" # What the user types in.\n",
    "result = await client.generate_text([secret_prompt + \" \" + prompt])\n",
    "result.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modal_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee83736e395fa6044e3c5a53f0c8b061053bac435b837e383b386743ab53bdde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
